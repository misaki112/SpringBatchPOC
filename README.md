# Spring Batch Data Importing POC
## **Introduction**
This POC proves Spring Batch's capability to 
1. Read from **multiple data sources** such as csv files and web services 
2. Write to databases such as MySQL 
3. Provide features of easy to **scale, restart, manage and monitor**.

## **What is Spring Batch?**
[**Spring Batch**](https://spring.io/projects/spring-batch) is an **open source** framework for **batch processing**.  
**Batch process** is the process to read data from external sources, process data and write data to desired output destination.   

## **Scenario**
- As our business grows, we are getting more hotel data in different kinds of form from various vendor sources.   
We want to load data from multiple sources provided by different vendors in an efficient way.
- We have applications doing this process for us, but the current data importing applications :  
1. Has huge classes which are not easy to understand.   
2. Business logic not very clear.   
3. Not easy to scale and parallelize.   
4. Not easy for monitoring and managing failure during execution.

## **Why Spring Batch?**
Spring Batch offers features below which solves the problems above,  
- Provides well defined **pattern** for batch process and **built-in** readers and writers   
which we can use out of the box and make implementation easier, business logic clearer. 
- Generate **META-DATA table** with metrics, such as execution time and exit status, of job and step executions which **enables monitoring** the batch process
- Provides Batch **management tool Spring Cloud Data Flow** which can be easily wired together with **exist** Spring Batch Jobs and has a UI dashboard for reading information from META-DATA tables generated during batch process.
- Batch process can be sped up by using built-in features such as **multi-threading, parallel steps and partitioning**.   
More scaling methods such as remote chunking can be configured using Spring Integration.

Here we use ***chunk-oriented step*** pattern, each data importing step consists of 3 parts: ***reader, processor and writer***.  
- **Reader** transform source information into java object for processing.  
- **Processor** takes the java object generated by Reader and process it, such as formatting, filling in data and map it to another Object for writing.  
- **Writer** takes in the output java object from Processor, transform the Object to output source.  

***Example of a chunk oriented step which imports data from a csv file*** 
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/Capture.PNG)

# **Proof of Concept**
## **Overview**  
### **Main Concepts**

| Main Concepts Used in POC | Tools Used | Brief Explanation | Benefit |
|---------------------------|------------|-------------------|-------|
|Multi-threading|Spring Batch's **TaskExecutor**|Run a **single step** with multiple threads at the same time|Scale to speed up batch process|
|Parallel Steps|Spring Batch's **Flow Split**|Run **multiple steps** with no dependency inside a job in a **parallel way**|Scale to speed up batch process|
|Restart|Spring Batch's **JobParameterIncrementer**|Restart a failed job at the **step it failed**|Save time need to restart a filed job after fixing errors|
|Retry|Spring Batch's **@Retry** annotation|Retry executing some code until reaches pre-set maxTries|Decrease job failure due to unstable web service connection|
|Partitioning|Spring Batch's **Partitioner interface**|Split a **master step** to several **slave steps** run by multiple threads independently|Scale to speed up batch process and enable failed csv reading step to run only the failed chunk when restarted|
|Auditing|Spring Batch's **Listeners and loggers**|Output information about the batch job into a log file|Keep track of how many percent a step is done and keep record of errors if any exception is thrown|
|Manage & Operation|Spring Batch's **META-DATA tables** and **Spring Cloud Data Flow**|Deploy application of batch process onto Spring Cloud Data Flow and extract information about execution time and other metrics|Run batch process from Spring Cloud Data Flow dashboard and get information about job execution on the UI. **Discover failure and reason of failure early**.|   

### **Data Flow**

| Data Source | How to Get Source Data| Data Destination |
|-------------|-----------------------|------------------|
|csv files|Query through AOLBkg-DEV.dev.costcotravel.com in SQL Server|MySQL Database|
|web service response|Generate soap response Json files from WSDL response by calling JDC|MySQL Database|  

### **Architecture**  
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/arcflow.PNG)

### **Page Guide**
- [**1. Read, process and write data using Spring Batch**](#Part-1-Read-process-and-write-data-using-Spring-Batch)
- [**2. Spring Batch Scaling for Large Data Set**](#Part-2-Working-with-Larger-Data-Set-and-Scaling-Batch-Process)
- [**3. Error Recovery of Batch Process with Spring Batch**](#Part-3-Improve-Error-Recovery-of-Batch-Process)
- [**4. Manage Batch Application with Spring Cloud Data Flow**](#Part-4-Use-Spring-Cloud-Data-Flow-to-Manage-Existing-Batch-Jobs)

## **Part 1. Read, process and write data using Spring Batch**
### Requirements
- IDE, Eclipse or Spring Tool Suite(STS)
- JDK 1.8 or later
- Maven 3.2+
- MySQL 8.0 **(When installing, set authentication to old one which support Spring Cloud Data Flow)**

### 1. Get Project with Required Dependencies from Spring Initializr
Use [**Spring Initializr**](https://start.spring.io/) for quick generating a project with correct structure and required dependencies in pom.xml file for Maven build.  

- Choose Java version and search for dependencies needed to add. 

|Dependency|Usage|   
|----------|-----|
|**Spring Batch**| Batch processing|     
|**MySQL Driver**| Connecting to MySQL|     
|**Spring Web Starter**| Reading from web services|    
|**Task**| Using Spring Cloud Data Flow|    

- When finished, click **Generate the project** to get jar file.  
- Unzip the project jar file and import into IDE as existing Maven project.  

### 2. Load Data from CSV Files to MySQL Database

**Assumption:**  
- Given [**csv files**](https://github.com/misaki112/SpringBatchPOC/tree/master/Spring-Batch-HOBE-POC-final-version/src/main/resources) representing data about hotels.The csv files used are generated from SQLServer database with connection to **AOLBkg-DEV.dev.costcotravel.com** through querying. 

**Difficulty:**   
- We want to write data in one row from input csv file into multiple tables in output SQL database.
- Spring Batch's chunk-oriented step has one-to-one relationship with reader, processor and writer.
- Spring Batch's reader, processor and writer cannot take inputs of different classes or return outputs of different classes.

**Solution:**  
- Create **intermediate row Object** to represent a whole row of data **while reading and processing**.   
- **Split data** from a **single row object** when mapping java object fields to columns in SQL tables, write them separately into different tables using different [**JdbcBatchItemWriter**](https://docs.spring.io/spring-batch/4.1.x/api/org/springframework/batch/item/database/JdbcBatchItemWriter.html)s in the writing process,   
- Combine multiple Writers to one [**CompositeItemWriter**](https://docs.spring.io/spring-batch/4.1.x/api/org/springframework/batch/item/support/CompositeItemWriter.html) to follow  Spring Batch's **one-to-one** relationship between **Step, Reader, Processor and Writer**.

**Data flow chart for batch process**

![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/csvReadingBatchProcess.png)

**Conclusion:**  
- Spring Batch is capable for reading data from multiple csv files and load them into one database.

### 3. Load Data from Web Service Response to Database

**Restriction:**  
- Load data given by calling web service into POJO using **only** feature in Spring Batch.  
- Avoid using manager classes from ***cwctravel.Core*** to **lower dependency** of the batch process. 

**Attempt1:**  
- Use Spring Batch built-in [**WebServiceTemplate**](https://docs.spring.io/spring-ws/sites/2.0/apidocs/org/springframework/ws/client/core/WebServiceTemplate.html) to transfer **wsdl files** returned by calling **JDC** into POJO stubs from Core.   

**Difficulty:**  
- Getting unavoidable "Cannot find dispatch method" exception because Spring Batch built-in WebServiceTemplate   
only support **Document binding style** while the WSDL of our JDC requests and response has **rpc binding style**. 

**Conclusion:**  
- Spring Batch is not capable for reading response from calling our JDC **without reconfiguring the binding style**   
of the wsdl files returned by the web service.

**Attempt2:**  
- Use **mock service** to mimic the responses can be get by calling **REST API**.

**Assumption:**  
- Get back a **json file** representing all information about hotels from REST API.   
(Json file generated from wsdl files by using [**xml to json converter**](https://codebeautify.org/xmltojson))
- Use SOAP UI's **Mock REST Service** to mimic the response of calling a REST API for simplicity.   
(Generate classes maps exactly the returning json file's structure by using [**JSON to Java Code Generator**](https://www.site24x7.com/tools/json-to-java.html) and adding missing classes manually)
- See [**here**](https://github.com/misaki112/SpringBatchPOC/tree/master/Spring-Batch-HOBE-POC-final-version/src/main/resources/REST_json_reponse) for json files using as response of the REST api.
- See [**this page**](https://shire.corp.costcotravel.com/display/ITKB/8.+SoapUI+Installation) and [**this page**](https://shire.corp.costcotravel.com/display/ITKB/SoapUI+Introduction) for instruction about SoapUI.

**Difficulty:**  
- Json Object for hotel has **nested structure** which is different from Spring Batch's feature for Reader, Writer and Processor that only take/return one type of object as input/output.   
- This makes it hard to let the batch job write to **all tables** about **one single hotel at one pass**.   
We are more likely to write same field of different hotels rather than different fields of the same hotel at a time following Spring Batch's step logic.
- But we prefer hotel **Object oriented** step rather than hotel **field oriented** step because it is easier to see which hotel has wrong data if error occurs.

**Solution:**  
- Let Reader take the most outer level object(Hotel) as input, Processor process that object **as a whole**.   
- For Writers, customize them to resemble the nested structure of the object by letting the **outer level writer trigger inner level writers** after writing output.

**Data flow chart for batch process**
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/DerbyBatchProcess.png)

**Conclusion:**  
- Spring Batch is capable for reading data from multiple REST services and load them into one database. 

### 4. Read Data from both Csv Files and Web services  

- Simply combining the two existing csv reading and web service reading file. Reconfigure the Batch Job to execute them sequentially.

**Conclusion:**  
- Spring Batch is capable for reading data from multiple sources including csv files and web services and load data into one database.   
- We can see Spring Batch makes the batch process easy to reconfigure and build. We build a multiple sources data importing process simply by combining 2 existing process.

## **Part 1.1. Versioning Data Written to Database**

**Scenario:**  
- We are reading from multiple sources and there are chances that data from different sources overlaps.   
So we need to avoid repeating primary keys in tables and keep track of where the data is from for clarity.

**Solution:**  
- Add ```Source``` column to tables to keep track of source of a piece of data, "csvFile" or "Derby"  
- Use MySQL query ```INSERT INTO ... ON DUPLICATE KEY UPDATE...``` to avoid repeated primary keys in ipm_hotel table

**Conclusion:**   
- Spring Batch is capable for reading data from multiple sources including csv files and web services and load data into one database even if data from different sources has overlap. 

## **Part 1.2. Reconfigure Database Structure**

**Scenario:**  
- When reading from Derby web service response, found that a lot of data is not used in current database structure.  
And some tables can be replaced by adding entry to parent tables.

**Solution:**  
- Reconfigured the HOBE Content in the following way.
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/_Blank%20ERD%20_%20Data%20Flow.png)
- Map Derby feature code to feature ID in our system.   
- Use a reference table to access feature type instead of using mapping tables.

**Result:**
- Data stored in a more **uniform and compact** way and reduce number of queries needed for data access.

## **Part 2. Working with Larger Data Set and Scaling Batch Process**
**Scenario:**  
- The data set used in previous parts are small with only around 100 hotel entries for proof of concept.  
- To see how Spring Batch performs with data of size closer to production, we need a larger data set and do scaling experiment on batch process.

#### Generally, there are 2 ways to scale a Spring batch job.   
1. Run batch job in a **multi-threading** way by using Spring Batch's built-in [**TaskExecutor**](https://docs.spring.io/spring/docs/current/javadoc-api/org/springframework/core/task/TaskExecutor.html).
2. Run batch job in a **multi-processing** way by using [**Spring Integration**](https://spring.io/projects/spring-integration).
See [**this page**](https://docs.spring.io/spring-batch/4.1.x/reference/html/scalability.html) for more information on scaling Spring Batch process.

***In this POC, we use Spring Batch's built in feature for scaling without using Spring Integration***

### Spring Batch Scaling Experiment with Multi-threads
#### Introduction
- Use all the hotel data we have by connecting to **AOLBkg-DEV.dev.costcotravel.com** and query for [**csv files**](https://github.com/misaki112/SpringBatchPOC/tree/master/Spring-Batch-HOBE-POC-final-version/src/main/resources).

- In total there are    
8265 **hotels**,   
173,443 **hotel rates**,   
5421 **hotel rooms**,  
133,497 **hotel room features**,   
247,668 **hotel images and urls**,   
112,292 **hotel features**,   
68,092 **hotel point of interests** to be load. 

- Use **Spring Cloud Data Flow** and **VisualVM** to check properties for each job instance using different scaling methods.  
***(Detail about these 2 tools will be in part 4 and 5 of this documentation)***

- Set **chunk_size=2000** for all cases, and **throttleLimit=10** for multi-threading.     
**chunk_size**: Number of read and write a step does before committing change to destination output.     
**throttleLimit**: Number of threads to use in multi-threading 

#### 1. Sequential Step Execution Without Scaling
- Total Time: 8 min 15 sec
- VisualVM Overview 
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/largeSeq.PNG)
- Step Details on Spring Cloud Data Flow
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/stepSeq.PNG)

#### 2. Parallel Steps with Flow Split
**Flow Split** is a Spring Batch built-in property which enables parallel steps using [**FlowBuilder**](https://docs.spring.io/spring-batch/trunk/apidocs/org/springframework/batch/core/job/builder/FlowBuilder.SplitBuilder.html)

After Flow Split, the application executes in the following way
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/Blank%20Diagram.png)

- Total Time: 5 min 48 sec

- VisualVM Overview 
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/largePara.PNG)  

- Step Details on Spring Cloud Data Flow
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/parastep.PNG)  

**Conclusion:**  
- Parallel steps enable a processor to switch between threads executing different steps.  
- Execution process of steps overlaps, total time needed decreases.
- The total running time is depending on the step which takes longest time in the parallel steps.

#### 3. Multi-threading using TaskExecutor
[**TaskExecutor**](https://docs.spring.io/spring/docs/current/javadoc-api/org/springframework/core/task/TaskExecutor.html) is a Spring Batch built-in feature which enables multi-threading.

A **single step** is executed with multiple threads after configuring.

**Throttel limit** is a parameter which decides how many threads a step is split into.

- Total Time: 3 min 59 sec

- VisualVM Overview
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/multiLarge.PNG)

- Step Details on Spring Cloud Data Flow
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/multiStep.PNG)

**Conclusion:**  
- Multi-threading speeds up the batch process by speeding up execution of each step.
- The total running time is the sum of time taken for each step.

#### 4. Combining Parallel Steps and Multi-threadings

With throttleLimit = 10, also run the steps in parallel way

- Total Time: 3 min 42 sec

- VisualVM Overview
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/bothLarge.PNG)

- Step Details on Spring Cloud Data Flow
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/bothSteps.PNG)

**Conclusion:**
- Both scaling methods are using one process to switch between threads, speeding up is not as obvious as applying each method to sequential process separately
- Since there are more threads than before, CPU usage and memory usage increases.

### Conclusion
#### By applying Spring Batch's built-in scaling methods to existing batch process, the execution time can be reduced by more than 50% of the sequential execution time.

## **Part 3. Improve Error Recovery of Batch Process**
#### **Scenario:**
- There are **deadlock** and **lock waiting timeout** exception occur when running the batch job by multi-threading and **errors appear randomly**.
- There can be **connection error** when calling **web service** due to **unstable network connection** which can be fixed by just wait and try later.   
- We have large data and it takes **too long to restart the whole job** from the first step every time we want to re-run a failed job.

#### **Problem 1: Database Lock Problems when Writing in Multiple Threads**
**Analysis:**  
- The deadlock appears even when the SQL queries only have ```INSERT``` instruction in them. The lock is a write-write lock.
- Run ```SHOW ENGINE INNODB STATUS``` in MySQL shell, get information about the deadlock. The lock type is [**Insertion Intention Lock**](https://dev.mysql.com/doc/refman/8.0/en/innodb-locking.html#innodb-insert-intention-locks) which is a special kind of [**Gap Lock**](https://dev.mysql.com/doc/refman/8.0/en/innodb-locking.html#innodb-gap-locks).
- **Gap Lock** appears when multiple threads are writing to the same row at the same time and can not be avoid because it is keeps the database synchronized and prevents **Phantom Read**.

**Solution:**  
- Notice that **Gap Lock** created when multiple thread writing to the same row at the same time. 
- So when a thread encounters **deadlock** caused by gap lock, use **try catch retry pattern** to catch deadlock exception and retry after 0.5 second so 2 threads not writing to same row at the same time.
- Set a maxTries number to prevent infinite loop.

    ***Try Catch Retry Patter in Customized Writer Extending JdbcBatchItemWritern***
    ```
    int count = 0;
    int maxTries = 10;

    //use while loop for retry
    while(true) {
        try {
            super.write(items);
            //break if success
            break; 
        } catch(DeadlockLoserDataAccessException e) {
            //let a thread wait for 0.5 second is deadlock appears
            Thread.sleep(50);
            if(++count == maxTries) {
                throw e;
            }
        }
    }
    ```
- **Lock Waiting Timeout Exception** can be solved in the same way.

#### **Problem 2: Web Service Connection Problem Due to Unstable Connection**
**Analysis:**  
- The reading process from web service may fail due to unstable connection.
- Retry reading from web service to avoid job failure due to random web service disconnection.

**Solution:**  
- Spring Batch has built-in [**Retry**](https://docs.spring.io/spring-batch/4.1.x/reference/html/retry.html) function which can restart a step on certain failure.
- Use ```@Retryable``` annotation for the **read method** inside the web service response reader.

    ```
    @Retryable(include = {Exception_Type.class}, maxAttempts = max_retry_times,  
    backoff = @Backoff(delay = wait_time_before_next_try, maxDelay = max_wait_time))
    ```   
- Add ```@EnableRetry``` to main application class to enable retry.

#### **Problem 3: Restart a Job at the Step of Failure**
**Analysis:**  
- A **FAILED** job can be restart as a new job instance at the step where the last job instance failed if passing in **same job parameters**.
- Currently using a Spring Batch's built-in [**RunIdIncrementer**](https://docs.spring.io/spring-batch/trunk/apidocs/org/springframework/batch/core/launch/support/RunIdIncrementer.html) to keep job parameter (runId) different for each job instance to **prevent restarting job** that has been executed.
- **RunIdIncrementer** is a special [**JobParametersIncrementer**](https://docs.spring.io/spring-batch/trunk/reference/htmlsingle/#JobParametersIncrementer) and is implemented in the following way.
    ```
    public class SampleIncrementer implements JobParametersIncrementer {

        public JobParameters getNext(JobParameters parameters) {
            if (parameters==null || parameters.isEmpty()) {
                return new JobParametersBuilder().addLong("run.id", 1L).toJobParameters();
            }
            long id = parameters.getLong("run.id",1L) + 1;
            return new JobParametersBuilder().addLong("run.id", id).toJobParameters();
        }
    }
    ```

**Solution:**    
- Use a customized **RunIdIncrementer** which will only increase runId when last execution of job instance has ExitCode of **COMPLETED** or **UNKNOWN**.
- Use **JdbcTemplate** to query for the **ExitCode of last job instance** from **Meta-Data** of the batch process to decide whether increase runId or not.
- Implementation of CustomizeIncrementer: 
    ```
    public class CustomizeIncrementer implements JobParametersIncrementer {

        private JdbcTemplate template;

        public CustomizeIncrementer(DataSource dataSource) {
            super();
            this.template = new JdbcTemplate(dataSource);
        }

        @Override
        public JobParameters getNext(JobParameters parameters) {
            if(parameters == null || parameters.isEmpty()) {
                return new JobParametersBuilder().addLong("run.id", 1L).toJobParameters();
            }
            String getExitCode = "SELECT EXIT_CODE FROM batch_job_execution ORDER BY JOB_EXECUTION_ID DESC LIMIT 1";
            String exitCode = template.queryForObject(getExitCode, String.class);
            long id = parameters.getLong("run.id", 1L);
            if(exitCode.equals("COMPLETED") || exitCode.equals("UNKNOWN")) {
                id++;
            }
            return new JobParametersBuilder().addLong("run.id", id).toJobParameters();
        }

    }
    ```

**Result:**  
- When restarting a FAILED job, the COMPLETED steps will not be executed and will have following log information:  
```Step already complete or not restartable, so no action to execute```  
Following by execution detail of the COMPLETED step.

#### **Problem 4: Restart Reading from the Failed Row in CSV File**
**Scenario:**  
- If a step is reading from a **large csv file**, ideally restart should **skip rows that have been read and written**   
and start from the rows that have not been written successfully last time.

**Difficulty:**  
- Spring Batch's multi-threadings **skip the whole failure chunk** when restart.   
Because the **read count** information, which the step use for restart inside the META-DATA table **batch step execution context** is not written correctly, it **doesn't rollback** the whole chunk when failure.   
So when restart, the data which have been read but not processed and written are skipped and causes **data lost**.

**Solution:**  
- Instead of multi-threading, use Spring Batch's [**Partition**](https://docs.spring.io/spring-batch/4.1.x/reference/html/scalability.html#partitioning) for scaling.  
Partitioning separate chunks of **master step** into individual **slave steps**.   
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/partitioning-overview.png)

- Store information about the **startIndex and endIndex** of each chunk into **batch step execution context** table.   
Use ```@Value``` annotation to retrieve the information from execution context in the reader so that reader will know which row to start and stop.  

**Example:**    
- The step **loadReferenceTable** here is reading from a csv file with 895 rows.  
- Use partitioning with gridSize = 10 (10 threads), each thread read and write around 90 rows.  
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/partitioningExample.PNG)

- In **Step Execution Context**, information about which chunk of data each thread is processing is shown.  
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/stepExecutionContext.PNG)

**Result:**  
- When restarting, only **failed slave step** inside the master step is executed. Only the failed chunk will be read, process and write.
- With partitioning, since each thread deal with **not overlapping** chunks, occurrence of deadlock problem reduced.  
- Partitioning is especially good for large data and has higher efficiency and **less concurrency limitation** than multi-threadings.  
- VisualVM Overview with Partitioning gridSize = 10 (10 threads)
  ![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/partitioningSequential.PNG)
- Both using 10 threads, with same amount of data, partitioning takes 3 min 23 sec which is shorter than 3 min 42 sec of using both multi-threading and parallel steps.

## **Part 4. Use Spring Cloud Data Flow to Manage Existing Batch Jobs**
### **What is Spring Cloud Data Flow?**
>[**Spring Cloud Data Flow**](https://spring.io/projects/spring-cloud-dataflow) provides tools to create complex topologies for streaming and batch data pipelines. The data pipelines consist of Spring Boot apps, built using the Spring Cloud Stream or Spring Cloud Task microservice frameworks.

### **Why Spring Cloud Data Flow?**
Spring Cloud Data Flow provides a nice UI dashboard which can **create applications, run tasks, restart failed jobs** and show all information including start and end time, exit code of an **existing Spring Batch job**. The UI can be accessed as a REST API on local machine by running Spring Cloud Data Flow Server.

### Make Existing Batch Job Executable in Spring Batch Data Flow 
### Reconfigure Existing Spring Batch Job
#### 1. Make application callable from Spring Cloud Data Flow
- In the main class of the batch job, add ```@EnableTask``` annotation.  
- This class-level annotation tells Spring Cloud Task to bootstrap its functionality.   
It enables a ```TaskConfigurer``` that registers the application in a ```TaskRepository```.  

#### 2. Package batch job application to a Spring Boot jar
- Run command line as administrator, navigate to the root file of the project. Execute following Maven command:  
```mvnw package```  
- As result, in the **target** directory, a JAR file will be created

### Get Connection to Spring Cloud Data Flow
#### 1. Get jar files for Spring Cloud Data Flow Server 
- Run Spring Cloud Data Flow on local machine for simplicity. Since vm2012 does not support Docker,   
see instruction on [**this page**](https://dataflow.spring.io/docs/installation/local/manual/) to get Spring Cloud Data Flow Server and shell and run them locally. (Spring Cloud Skipper is optional)
- Or go to [**git repository of Spring Cloud Data Flow**](https://github.com/spring-cloud/spring-cloud-dataflow), pull source code. Import projects to IDE,   
configure database inside IDE and run main class in project **spring-cloud-dataflow-server** as Spring-Boot application

#### 2. Run Spring Cloud Data Flow Server with correct database configuration  
- The database used by Spring Cloud Data Flow should be the same as the database being used in the batch job.    
- Spring Cloud Data Flow's default database is H2. In the batch process built before, MySQL is used so here configure database to MySQL. 
- Run jar file with command line with following parameters to connect Spring Cloud Data Flow to MySQL database.

```
java -jar spring-cloud-dataflow-server-2.1.2.RELEASE.jar --spring.datasource.url=jdbc:mysql://localhost:3306/mydb --spring.datasource.username=myusername --spring.datasource.password=mypassword --spring.datasource.driver-class-name=org.mariadb.jdbc.Driver 
```

- Connect Spring Cloud Data Flow to the database the batch process will be using **before any batch process is run**.    
This avoid the concurrency issue between the tables created by Spring Cloud Data Flow and tables created by Spring Batch Process.  

See [**this page**](https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#configuration-local-rdbms) for configuring other databases

#### 3. Accessing Data Flow Dashboard
- After connection is made, in browser, navigate to [**Spring Cloud Data Flow Dashboard URL**](http://localhost:9393/dashboard).  
The default port Spring Cloud Data Flow uses on local machine is **http://localhost:9393/dashboard**

### Register Batch Job to Spring Cloud Data Flow
Open dashboard after connection, choose Apps on menu and click on **Add Application(s)** button  
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/addApp.PNG)

Choose **Register one or more applications**, fill in **name** for the application and select **Task** as **type**.  
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/registerapp.PNG)

For **URI**, enter the path of the location of the JAR file created by Maven build.  
For example, ```file://D:/spring-batch-poc/POC_SpringBatch/webServicePractice/target/webServicePractice-0.0.1-SNAPSHOT.jar```

Click on **Register the application(s)** when done. As result, the application will appear on the **Apps** list.

### Launch Instance of Batch Job
After registering  the batch job as application, go to **Tasks** on the menu, click **Create task(s)**
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/createTask.PNG)

Choose task from the drop down menu and drag it into the graph, connect **START** and **END** node to it and click **Create Task** to name the task instance.
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/createTask2.PNG)

Click the **Run** button of the application to be launched. Fill in Arguments and Parameters if needed (optional), then click **Launch the task**.
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/runTask.PNG)

After task finish, information about execution can be found on **Executions** page.
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/executionPage.PNG)

If the batch job generates log file, in the command line connection to Spring Cloud Data Flow, the directory the log file is in can be found.

### Getting Information About a Launched Task
After the task is finished, the status of the task can be found on the **Jobs** page.
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/jobPage.PNG)

Click on the task, a list of the task's properties can be found.
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/TaskProp.PNG)

Step detail information can be found when scroll down the page.
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/stepDetails.PNG)

If the job instance fails, the exact step which cause the failure can be found. 
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/failSstep.PNG)

Clicking into that step, error causing the failure will be shown.
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/failDetail.PNG)

**Conclusion:**  
- With **Spring Cloud Data Flow**, after adding annotation to **pre-existing** Spring Batch jobs, we can manage batch process easily.

**Note:**   
- As long as **Spring Cloud Data Flow** is connected to the database the batch process is using, information about a certain instance of the batch job **will be shown** even if we **did not register** the application and launch it locally rather than on Spring Cloud Data Flow. 
- In that case, the job will be shown as **No Task Definition**.


## **Part 5. Monitoring Metrics of Batch Process Using VisualVM**
**Scenario:**  
- By using Spring Cloud Data Flow, we have access to properties of a batch job such as start time, end time and exit code.  
- But we do not have access to other metrics such as **memory usage** and **thread number** of the batch job.  

**Difficulty:**  
- Spring Batch's built-in metric accessor can only access properties such as start time, end time which we can already get in Spring Cloud Data Flow.  

**Solution:**  
- Use Java Profiler Tool, [**VisualVM**](https://visualvm.github.io/) to monitor the metrics of the batch process application.  
- See [**this page**](https://blog.idrsolutions.com/2013/05/setting-up-visualvm-in-under-5-minutes/) for setting up VisualVM and connect it to Eclipse.

### **Analyzing Hotel Import Batch Job with VisualVM Experiment:**  
- Use the application of importing Hilton, Hyatt, Fairmont from Derby response and Hyatt, Fairmont from csv Files in **part 1.2**.
- Apply both parallel steps and multi-threading in each step with throttleLimit = 5.

**Summary of Hotel Import Application:**  
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/visualvmOverview.PNG)

**Analysis:**  
- From the graph, the batch job has a heavy use of the CPU at around 100% at the same time when the batch process has the maximum number of thread running.  
- As the process goes, some steps in the batch job finishes earlier and the threads decrease which also makes the usage of CPU decrease.
- For thread numbers, the maximum threads running together is 57 and total threads generated is 147.  
The batch job has 23 steps, in which 10 are chunk-oriented for reading and writing data and 13 are tasklet for cleaning  existing database  
tables. The 10 chunk-oriented steps are split into 5 thread each since throttleLimit is 5. 
- The maximum heap size of the batch process is 735MB and the maximum heap used is 529MB which is small.

**Change ThrottleLimit = 2, Summary of Hotel Import Application:**
![text](https://github.com/misaki112/SpringBatchPOC/blob/master/images/limit2overview.PNG)  

**Analysis:**   
- Comparing to the graph for throttleLimit = 5, the maximum CPU usage decrease from 100% to 79%.
- The maximum threads running at the same time and the total threads generated also decrease.
- Total time of batch job speed up by 1 second.

**Conclusion:**
- The more threads we have, the more percentage of the CPU will be used. Efficiency decreased when CPU too heavily used.  

## **Part 6: Unit Test for Spring Batch**
See [**this page**](https://docs.spring.io/spring-batch/4.0.x/reference/html/testing.html) about general information for testing Spring Batch applications  

### 1. Testing Step Components
**Scenario:**  
- We want to have tests to test each **component** in the batch process such as **readers, processors and writers** independently so it will be easier to find source of errors.

**Difficulty:**     
- Each component of a Spring Batch step is only run inside the scope of that step.   
- We will need to mock the environment of the step scope and input for the components.

**Solution:**
- Spring Batch provide us with ``@TestExecutionListeners`` and when we add   
```
@TestExecutionListeners( { DependencyInjectionTestExecutionListener.class,
    StepScopeTestExecutionListener.class })
```   
to the front of out test, the environment of executing that certain step will be created and we can create component for testing using ```@Autowired``` annotation

### 2. Testing a Step  
**Scenario:**  
- After we test each component of a step, we want to test that step to see if theses components work together in a correct way.   
**Solution:**  
- Spring Batch provides us [**JobLauncherTestUtils**](https://docs.spring.io/spring-batch/trunk/apidocs/org/springframework/batch/test/JobLauncherTestUtils.html) to launch a step independently when testing.     
- First we use ```@Autowired``` to generate the JobLauncherTestUtils that has the same configuration as our batch job, then we can use     
```JobExecution jobExecution = jobLauncherTestUtils.launchStep(name_of_the_step);```   
to run the step inside the test

### 3. Testing the Whole Job  
**Scenario:**
- After each steps in the batch job works correctly, we want to run an end-to-end test of the whole batch job    
to see if there is any dependency, concurrency and order issues between steps.

**Solution:**    
There are 2 ways to do it:     
1. When we download the package from Spring Initializr, a test class is generate for us and it is an end-to-end test of the whole application.
2. We can use **JobLauncherTestUtils** and launch the whole job by running   
```JobExecution jobExecution = jobLauncherTestUtils.launchJob();```

### 4. Mocking Calling Web Service in Unit Tests
**Scenario:**
- We want to test the readers reading from web service response in unit tests
- During tests, we do not want the real web service to be hit so we need to mock web service inside java  

**Solution:**
- Spring Boot provides us [**MockRestServiceServer**](https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/test/web/client/MockRestServiceServer.html) which can be used to mock web service response without actually calling the service.  

- Example:
```
@RunWith(SpringJUnit4ClassRunner.class)
@TestExecutionListeners({DependencyInjectionTestExecutionListener.class, StepScopeTestExecutionListener.class})
@ContextConfiguration(classes = {the set of batch configuration classes})
@TestPropertySource(locations = "classpath:application.properties")
public class ReaderTest extends TestCase {

	@Autowired
	private ItemReader<reader_output_object> reader;

	@Autowired
	private RestTemplate restTemplate;

    //the url the reader is calling to
	@Value("${url}")
	private String URL;

    // the response expected by calling the web service
    private String response = A_Json_File_formatted_String

    private MockRestServiceServer mockServer;

	@Override
	@Before
	public void setUp() {
		RestGatewaySupport gateway = new RestGatewaySupport();
		gateway.setRestTemplate(restTemplate);
		mockServer = MockRestServiceServer.createServer(gateway);
		this.mockServer.expect(requestTo(URL))//
				.andRespond(withSuccess(response, MediaType.APPLICATION_JSON));
	}

	@Test
	public void readerTest() throws Exception {
		reader_output_object item = this.reader.read();
		
        //do assertions about item here
        ...
	}
}
```
- See [**this page**](https://examples.javacodegeeks.com/enterprise-java/spring/using-mockrestserviceserver-test-rest-client/) for more informationtion  

### Conclusion:
Spring Batch offers us a lot of feature for testing batch applications without using any mocking plug-ins such as Mokitos which we will need to use in other cases.    

#### PS:  
If ***No JUnit test found*** error pop out when trying to run tests.   
1. First check **run configuration** of the test class to see if JUnit4 is being used  
2. Then check if the test file is inside **Build Path** of the project.   
3. If still doesn't work after checking the above, add ```extends TestCase``` to the end when declaring the test classes.  

# **Conclusion**
### Spring Batch can load HOBE content from multiple input sources to SQL database and make batch process easy to scale, reconfigure, restart, manage and monitor

# **Resources and Links**  

**Examples of Spring Batch' s Built-in Readers and Writers: https://www.petrikainulainen.net/spring-batch-the-ultimate-resource/**

**Spring Cloud Data Flow Guiding Site: https://dataflow.spring.io/docs/installation/local/**

**Git Repository of Spring Cloud Data Flow: https://github.com/spring-cloud/spring-cloud-dataflow**

**Spring Batch Guiding Page: https://spring.io/projects/spring-batch**

**VisualVM Home: https://visualvm.github.io/**

[**Presentation PowerPoint**](https://github.com/misaki112/SpringBatchPOC/blob/master/images/SpringBatchPOC%20-%20Copy.pptx) 

[**Video DEMO**](https://github.com/misaki112/SpringBatchPOC/blob/master/SpringBatchDEMO.mp4)
